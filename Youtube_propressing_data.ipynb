{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '443.csv'\n",
    "df = pd.read_csv(file_path, usecols = ['id','title','description','likecount','dislikecount','tags'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "words = set(nltk.corpus.words.words())\n",
    "words = {x.lower() for x in words}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "def rm_punctuation(sent):\n",
    "    return regex.sub('',sent)\n",
    "def rm_foreign(sent, min_words = 20):\n",
    "    original = sent\n",
    "    sent = sent.replace('\\r\\n', '')\n",
    "    sent = sent.replace('\\n', '')\n",
    "    sent = re.sub(r'href\\S+', '', sent)\n",
    "    check = \" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
    "         if (w.lower() in words) and (w.lower() not in stop_words))\n",
    "    # if new title or description contains too little infromation afte cleaning, drop it.\n",
    "    if len(check) / len(original) > 0.3:\n",
    "        if len(check) > min_words:\n",
    "            return check.lower()\n",
    "        else:\n",
    "            return None\n",
    "        return check.lower()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def tags_cleaning(tags):\n",
    "    tags_lst = tags.split(',')\n",
    "    new_set = set()\n",
    "    for tag in tags_lst:\n",
    "        new_tag = ''\n",
    "        for w in nltk.wordpunct_tokenize(tag):\n",
    "            if (w.lower() in words) and (w.lower() not in stop_words):\n",
    "                new_tag += w \n",
    "                new_tag += \" \"\n",
    "        if len(new_tag.strip()) / len(tag) < 0.5:\n",
    "            new_tag = ''\n",
    "        new_tag = stem_condense(new_tag)\n",
    "        if new_tag !='' and (not new_tag.isspace()): # drop all tags that is none or only contain spaces .\n",
    "            new_set.update({new_tag.strip().lower()})\n",
    "        \n",
    "    if len(new_set) == 0:\n",
    "        return None\n",
    "    return list(new_set)[0:5]\n",
    "\n",
    "# generate synonyms dictionary \"dic\"\n",
    "syn = pd.read_csv('synonyms.csv') # this file is from github project\n",
    "dic = dict()\n",
    "for ind in syn.index:\n",
    "    row = syn.iloc[ind,:]['words'].split(',')\n",
    "    for i in range(1,len(row)):      \n",
    "        dic[row[i].strip()] = row[0]\n",
    "\n",
    "def stem_condense(sent):\n",
    "    new_sent = []\n",
    "    for w in word_tokenize(sent):\n",
    "        #condense\n",
    "        condensed_w = dic.get(w,w)\n",
    "        #stem\n",
    "        stem_w = ps.stem(condensed_w)\n",
    "        new_sent.append(stem_w)\n",
    "    new_sent = ' '.join(new_sent)\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should only be run ONCE!!\n",
    "df['description'] = df['description'].apply(rm_foreign)\n",
    "df = df.dropna()\n",
    "df['description'] = df['description'].apply(rm_punctuation)\n",
    "df['tags'] = df['tags'].apply(tags_cleaning)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write in a csv file.\n",
    "# df.to_csv('cleaned_condensed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write in a csv file.\n",
    "# df.to_csv('cleaned_condensed.csv')\n",
    "import csv\n",
    "df.head()\n",
    "header=df['tags']\n",
    "result=[]\n",
    "total_rows = df['id'].count()\n",
    "\n",
    "result.append(['Id','Title','Description','LikeCount','DislikeCount','Condensed Tags (comma delimited string)'])\n",
    "for i in range(total_rows):\n",
    "    result.append([df['id'][i],df['title'][i],df['description'][i],df['likecount'][i],df['dislikecount'][i]])\n",
    "    for j in range(len(df['tags'][i])):\n",
    "        result[i+1].append(df['tags'][i][j])\n",
    "with open('datafile.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
